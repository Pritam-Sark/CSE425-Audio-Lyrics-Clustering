Multimodal Audio-Lyrics Clustering with Beta-VAE

 ðŸ“Œ Project Overview
This project implements a Deep Learning approach to cluster music tracks by fusing Audio Spectrograms and Lyrics. 



 ðŸ“‚ Project Structure
The file structure follows the project requirements:

```text
CSE425-Audio-Lyrics-Clustering/
â”œâ”€â”€ data/                       # Dataset (Audio .mp3 and Lyrics .csv)
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Easy_Task.ipynb         # Baseline Linear VAE (Audio Only)
â”‚   â”œâ”€â”€ Medium_Task.ipynb       # ConvVAE + Hybrid Clustering
â”‚   â””â”€â”€ Hard_Task.ipynb         # Beta-VAE + Advanced Metrics (Main Entry Point)
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ latent_visualization/   # Generated plots (Latent space, Purity, Reconstruction)
â”‚   â”œâ”€â”€ clustering_metrics.csv  # Final comparison table
â”‚   â””â”€â”€ beta_vae_model.pth      # Saved model weights
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ vae.py                  # PyTorch Model Definitions (ConvVAE, Beta-VAE)
â”‚   â”œâ”€â”€ dataset.py              # Custom PyTorch Dataset Loaders
â”‚   â”œâ”€â”€ clustering.py           # Clustering Logic (KMeans, DBSCAN)
â”‚   â””â”€â”€ evaluation.py           # Metrics (Purity, NMI, ARI)
â””â”€â”€ README.md