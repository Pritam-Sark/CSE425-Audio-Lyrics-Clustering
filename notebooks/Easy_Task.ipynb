{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96159cb7",
   "metadata": {},
   "source": [
    "Imports & Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d06ffdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add 'src' to python path so we can import our files\n",
    "sys.path.append('../src')\n",
    "from dataset import AudioDataset\n",
    "from vae import VAE\n",
    "\n",
    "# Configuration\n",
    "# NOTE: Ensure these paths match your actual folders\n",
    "AUDIO_DIR = '../data/audio'  \n",
    "CSV_PATH = '../data/fma_lyrics_dataset.csv'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-3\n",
    "EPOCHS = 10  # Set to 10-20 for a quick test\n",
    "LATENT_DIM = 32\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Running on: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a70d7f",
   "metadata": {},
   "source": [
    "Load Data (The Moment of Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788e782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset... this might take a minute to check file paths.\n",
      "Dataset initialized. Found 3375 valid audio files out of 3375.\n",
      "Input Feature Size: 16796\n"
     ]
    }
   ],
   "source": [
    "# Initialize Dataset\n",
    "print(\"Initializing dataset... this might take a minute to check file paths.\")\n",
    "dataset = AudioDataset(CSV_PATH, AUDIO_DIR, duration=30)\n",
    "\n",
    "if len(dataset) == 0:\n",
    "    raise ValueError(\"No files found! Check your AUDIO_DIR path and folder structure.\")\n",
    "\n",
    "# Create Loader\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Check one sample to set input dimensions\n",
    "sample_data = dataset[0]\n",
    "INPUT_DIM = sample_data.shape[0]\n",
    "print(f\"Input Feature Size: {INPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640397cf",
   "metadata": {},
   "source": [
    "Train the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f17df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  23%|██▎       | 24/106 [01:21<04:44,  3.47s/it, loss=0.361]d:\\CSE425 Project\\notebooks\\../src\\dataset.py:47: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(file_path, sr=self.target_sample_rate, duration=self.duration)\n",
      "d:\\CSE425 Project\\venv\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
      "Epoch 1/10: 100%|██████████| 106/106 [06:00<00:00,  3.40s/it, loss=0.322]\n",
      "Epoch 2/10: 100%|██████████| 106/106 [05:27<00:00,  3.09s/it, loss=0.285]\n",
      "Epoch 3/10: 100%|██████████| 106/106 [05:29<00:00,  3.11s/it, loss=0.295]\n",
      "Epoch 4/10: 100%|██████████| 106/106 [05:27<00:00,  3.09s/it, loss=0.208]\n",
      "Epoch 5/10:  82%|████████▏ | 87/106 [04:29<00:59,  3.14s/it, loss=0.234]"
     ]
    }
   ],
   "source": [
    "model = VAE(input_dim=INPUT_DIM, latent_dim=LATENT_DIM).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = loss_fn(recon_x, x)\n",
    "    # KL Divergence term\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return MSE + 0.0001 * KLD\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "model.train()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        batch = batch.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(batch)\n",
    "        loss = loss_function(recon_batch, batch, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    losses.append(avg_loss)\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(losses, label='Training Loss')\n",
    "plt.title(\"VAE Training Progress\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8334ed",
   "metadata": {},
   "source": [
    "Clustering & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f69648",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_latents = []\n",
    "\n",
    "print(\"Extracting latent features...\")\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        batch = batch.to(DEVICE)\n",
    "        _, mu, _ = model(batch)\n",
    "        all_latents.append(mu.cpu().numpy())\n",
    "\n",
    "X_latent = np.concatenate(all_latents, axis=0)\n",
    "\n",
    "# K-Means Clustering\n",
    "print(\"Performing K-Means Clustering...\")\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_latent)\n",
    "\n",
    "# Calculate Scores\n",
    "sil_score = silhouette_score(X_latent, cluster_labels)\n",
    "print(f\"VAE + K-Means Silhouette Score: {sil_score:.4f}\")\n",
    "\n",
    "# Visualize with t-SNE\n",
    "print(\"Running t-SNE for visualization...\")\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_latent)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x=X_tsne[:,0], y=X_tsne[:,1], hue=cluster_labels, palette=\"viridis\", s=50)\n",
    "plt.title(f\"VAE Latent Space (Silhouette: {sil_score:.2f})\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
